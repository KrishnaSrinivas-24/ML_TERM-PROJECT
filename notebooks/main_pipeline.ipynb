{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "034510f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading and Preprocessing Data ---\n",
      "Found 10 classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
      "Data loading complete. Took 535.43 seconds.\n",
      "Total images loaded: 27000\n",
      "Shape of image data: (27000, 64, 64, 3)\n",
      "Shape of labels: (27000,)\n",
      "------------------------------\n",
      "\n",
      "--- Step 2: Extracting Features and Splitting Data ---\n",
      "Feature extraction complete. Shape of feature data: (27000, 96)\n",
      "Data splitting complete. Took 3.57 seconds.\n",
      "Shape of X_train: (21600, 96)\n",
      "Shape of X_test: (5400, 96)\n",
      "------------------------------\n",
      "\n",
      "--- Step 3: Applying PCA for Dimensionality Reduction ---\n",
      "PCA application complete. Took 0.55 seconds.\n",
      "Original feature shape: (21600, 96)\n",
      "Shape after PCA: (21600, 50)\n",
      "PCA object saved to 'e:\\GITAM SEMESTERS\\5th-sem\\MLA - CSEN3261\\ML_TERM-PROJECT\\ML_TERM-PROJECT\\notebooks\\..\\models\\pca.joblib'\n",
      "------------------------------\n",
      "\n",
      "--- Step 4: Saving Processed Data ---\n",
      "All processed data has been saved to the '../results' folder.\n",
      "You are now ready to run the 'model_training.ipynb' notebook.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "# --- Step 1: Data Loading and Preprocessing ---\n",
    "print(\"--- Step 1: Loading and Preprocessing Data ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the path to your data directory relative to the notebook's location\n",
    "data_dir = '../data'\n",
    "\n",
    "# Get the list of class names from the folder names\n",
    "try:\n",
    "    class_names = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The directory '{data_dir}' was not found. Please ensure your data is in the correct folder.\")\n",
    "    # Exit the notebook gracefully if data directory is not found\n",
    "    exit()\n",
    "    \n",
    "print(f\"Found {len(class_names)} classes: {class_names}\")\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Loop over each class folder\n",
    "for class_name in class_names:\n",
    "    class_path = os.path.join(data_dir, class_name)\n",
    "    image_files = os.listdir(class_path)\n",
    "    \n",
    "    # Loop over each image in the class folder\n",
    "    for image_file in image_files:\n",
    "        if image_file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_path = os.path.join(class_path, image_file)\n",
    "            \n",
    "            # Read the image using OpenCV\n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            if image is not None:\n",
    "                # OpenCV reads images in BGR format, convert it to RGB for consistency\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Resize the image to a standard 64x64\n",
    "                image = cv2.resize(image, (64, 64))\n",
    "                \n",
    "                # Normalize pixel values to be between 0 and 1\n",
    "                image = image.astype('float32') / 255.0\n",
    "                \n",
    "                images.append(image)\n",
    "                labels.append(class_name)\n",
    "\n",
    "# Convert lists to NumPy arrays for efficient processing\n",
    "images = np.array(images)\n",
    "\n",
    "# Encode text labels to numbers\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Data loading complete. Took {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Total images loaded: {len(images)}\")\n",
    "print(f\"Shape of image data: {images.shape}\")\n",
    "print(f\"Shape of labels: {labels_encoded.shape}\")\n",
    "print(\"------------------------------\\n\")\n",
    "\n",
    "\n",
    "# --- Step 2: Feature Extraction (Color Histograms) ---\n",
    "print(\"--- Step 2: Extracting Features and Splitting Data ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "features = []\n",
    "for img in images:\n",
    "    # Calculate histograms for each color channel\n",
    "    hist_r = cv2.calcHist([img], [0], None, [32], [0, 1])\n",
    "    hist_g = cv2.calcHist([img], [1], None, [32], [0, 1])\n",
    "    hist_b = cv2.calcHist([img], [2], None, [32], [0, 1])\n",
    "    \n",
    "    # Concatenate the histograms and flatten them to create a single feature vector\n",
    "    features.append(np.concatenate((hist_r, hist_g, hist_b)).flatten())\n",
    "\n",
    "features = np.array(features)\n",
    "print(f\"Feature extraction complete. Shape of feature data: {features.shape}\")\n",
    "\n",
    "# Split the dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, \n",
    "    labels_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=labels_encoded\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Data splitting complete. Took {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(\"------------------------------\\n\")\n",
    "\n",
    "\n",
    "# --- Step 3: Dimensionality Reduction with PCA ---\n",
    "print(\"--- Step 3: Applying PCA for Dimensionality Reduction ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize PCA to reduce dimensions to 50\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "\n",
    "# Fit PCA on the training data and transform both training and test data\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Define paths for saving models and results\n",
    "base_path = os.getcwd() # Gets the current notebook directory\n",
    "models_path = os.path.join(base_path, '..', 'models')\n",
    "os.makedirs(models_path, exist_ok=True) # Ensure the directory exists\n",
    "\n",
    "# Save the fitted PCA object so we can use it in the app\n",
    "pca_save_path = os.path.join(models_path, 'pca.joblib')\n",
    "joblib.dump(pca, pca_save_path)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"PCA application complete. Took {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Original feature shape: {X_train.shape}\")\n",
    "print(f\"Shape after PCA: {X_train_pca.shape}\")\n",
    "print(f\"PCA object saved to '{pca_save_path}'\")\n",
    "print(\"------------------------------\\n\")\n",
    "\n",
    "\n",
    "# --- Step 4: Saving Processed Data for the Next Notebook ---\n",
    "print(\"--- Step 4: Saving Processed Data ---\")\n",
    "\n",
    "# Define path for saving processed data\n",
    "results_path = os.path.join(base_path, '..', 'results')\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "# Save the final NumPy arrays to be used by the model training notebook\n",
    "np.save(os.path.join(results_path, 'X_train_pca.npy'), X_train_pca)\n",
    "np.save(os.path.join(results_path, 'X_test_pca.npy'), X_test_pca)\n",
    "np.save(os.path.join(results_path, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(results_path, 'y_test.npy'), y_test)\n",
    "np.save(os.path.join(results_path, 'class_names.npy'), label_encoder.classes_)\n",
    "\n",
    "print(\"All processed data has been saved to the '../results' folder.\")\n",
    "print(\"You are now ready to run the 'model_training.ipynb' notebook.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
